{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This notebook needs pymc and arviz to be installed\n",
    "# in the used environment. Either install them by the method\n",
    "# of your choice or uncomment the code below and run it.\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install arviz pymc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescoring Using Bayesian and Frequentist Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After the initial scoring of Peptide Spectrum Matches by sage, additional information\n",
    "such as the deviation of the spectrum's ion mobility or retention time to those predicted for\n",
    "the peptide can be utilized to refine the scoring. This process is called rescoring.\n",
    "Here, a statistical model or machine learning approaches are trained to predict wether the peptide\n",
    "behind a PSM is a target or a decoy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from typing import List, Tuple, Optional\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.style.use(\"bmh\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# The features we want to use for rescoring\n",
    "\n",
    "features = [\n",
    "    \"score\",\n",
    "    \"delta_rt\",\n",
    "    \"delta_ims\",\n",
    "    \"cosine_similarity\",\n",
    "    \"delta_mass\",\n",
    "    \"rank\",\n",
    "    \"isotope_error\",\n",
    "    \"average_ppm\",\n",
    "    \"delta_next\",\n",
    "    \"delta_best\",\n",
    "    \"matched_peaks\",\n",
    "    \"longest_b\",\n",
    "    \"longest_y\",\n",
    "    \"longest_y_pct\",\n",
    "    \"missed_cleavages\",\n",
    "    \"matched_intensity_pct\",\n",
    "    \"poisson\",\n",
    "    \"charge\",\n",
    "    \"intensity_ms1\",\n",
    "    \"intensity_ms2\",\n",
    "    \"collision_energy\",\n",
    "    \"spectral_entropy_similarity\",\n",
    "    \"spectral_correlation_similarity_pearson\",\n",
    "    \"spectral_correlation_similarity_spearman\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_whole = pd.read_csv(\"./psm_data.csv\", usecols=features+[\"decoy\",\"match_idx\"])\n",
    "# for the sake of runtime of the bayesian model, we will sample 1% of the data\n",
    "data_shuffle = data_whole.sample(frac=1)\n",
    "data_downsampled = data_shuffle.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_idx_train = []\n",
    "match_idx_test = []\n",
    "\n",
    "size_data = data_downsampled.shape[0]\n",
    "frac_train = 0\n",
    "for mi in data_downsampled.match_idx.unique():\n",
    "    if frac_train < 0.5:\n",
    "        match_idx_train.append(mi)\n",
    "        frac_train = data_downsampled.loc[data_downsampled.match_idx.isin(match_idx_train)].shape[0]/size_data\n",
    "    else:\n",
    "        match_idx_test.append(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_train = data_downsampled[data_downsampled.match_idx.isin(match_idx_train)]\n",
    "data_test = data_downsampled[data_downsampled.match_idx.isin(match_idx_test)]\n",
    "\n",
    "y_train = data_train[\"decoy\"].apply(lambda x: 1 if x else 0)\n",
    "X_train = data_train.loc[:, features]\n",
    "\n",
    "y_test = data_test[\"decoy\"].apply(lambda x: 1 if x else 0)\n",
    "X_test = data_test.loc[:, features]\n",
    "# Standardize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_rescoring = (data_test.assign(decoy= lambda x: x[\"decoy\"].astype(int), \n",
    "            score = lambda x: -x[\"score\"],\n",
    "            ).sort_values(\"score\")\n",
    "            .assign(percent_decoy= lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "            .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "cutoff = before_rescoring.where(before_rescoring[\"percent_decoy\"] > 1).dropna().head(1)\n",
    "sns.histplot(before_rescoring, x=\"score\",ax=ax1,hue=\"decoy\")\n",
    "sns.lineplot(data=before_rescoring, x=\"score\", y=\"percent_decoy\",ax=ax2, color=\"red\")\n",
    "ax1.vlines(cutoff[\"score\"], 0, 600, color=\"black\", linestyle=\"--\")\n",
    "ax1.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"score\"].values[0]-10, 600), color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"peptides\": X_train.index.values,\"features\": X_train.columns}) as logistic_model:\n",
    "    x_data = pm.Data(\"x_data\", X_train_scaled)\n",
    "    y_data = pm.Data(\"y_data\", y_train)\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    beta = pm.Normal(\"beta\", mu=0 , sigma=0.1, shape=X_train_scaled.shape[1], dims=[\"features\"])\n",
    "\n",
    "    logit_p = pm.Deterministic(\"logit_p\", alpha + pm.math.dot(x_data, beta), dims=[\"peptides\"])\n",
    "    obs = pm.Bernoulli(\"obs\", logit_p=logit_p, observed=y_data, dims=[\"peptides\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from the Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with logistic_model as model:\n",
    "    idata = pm.sample(init=\"adapt_diag\")\n",
    "    pm.set_data({\"x_data\": X_test_scaled, \"y_data\": y_test})\n",
    "    model.set_dim(\"peptides\", X_test_scaled.shape[0], X_test.index.values)\n",
    "    model.set_dim(\"features\", X_test_scaled.shape[1], X_test.columns)\n",
    "    pm.sample_posterior_predictive(idata, var_names=[\"obs\",\"logit_p\"], predictions=True, extend_inferencedata=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"alpha\", \"beta\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = idata.posterior.beta.mean(dim=[\"chain\",\"draw\"]).to_dataframe()\n",
    "sns.barplot(data=feature_importances, x=\"beta\", y=\"features\")\n",
    "plt.xlabel(\"Posterior Mean of Logistic Regression Coefficients\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importances (PyMC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoy_peptides = data_test.loc[data_test[\"decoy\"],:].index\n",
    "target_peptides = data_test.loc[~data_test[\"decoy\"],:].index\n",
    "posterior_decoy = idata.predictions.logit_p.sel(peptides=decoy_peptides).mean(dim=[\"chain\",\"draw\"]).values.flatten()\n",
    "posterior_target = idata.predictions.logit_p.sel(peptides=target_peptides).mean(dim=[\"chain\",\"draw\"]).values.flatten()\n",
    "\n",
    "after_rescoring_pymc = (pd.DataFrame({\"mean_logit_p\": np.concatenate([posterior_decoy, posterior_target]),\n",
    "                               \"decoy\": np.concatenate([np.ones_like(posterior_decoy), np.zeros_like(posterior_target)])})\n",
    "                    .sort_values(\"mean_logit_p\")\n",
    "                    .assign(percent_decoy=lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "                    .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "cutoff = after_rescoring_pymc.where(after_rescoring_pymc[\"percent_decoy\"] > 1).dropna().head(1)\n",
    "sns.lineplot(data=after_rescoring_pymc, x=\"mean_logit_p\", y=\"percent_decoy\",ax=ax2, color=\"red\")\n",
    "sns.histplot(after_rescoring_pymc, x=\"mean_logit_p\",ax=ax1,hue=\"decoy\")\n",
    "ax1.vlines(cutoff[\"mean_logit_p\"], 0, 660, color=\"black\", linestyle=\"--\")\n",
    "ax1.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"mean_logit_p\"].values[0]-5, 660), color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l2\", C=0.01)\n",
    "\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "logit_p = clf.decision_function(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_rescoring_sklearn = (data_test.assign(decoy= lambda x: x[\"decoy\"].astype(int),\n",
    "                                            logit_p_sklearn = logit_p\n",
    "            ).sort_values(\"logit_p_sklearn\")\n",
    "            .assign(percent_decoy= lambda x: x[\"decoy\"].cumsum() / x[\"decoy\"].sum() * 100)\n",
    "            .assign(count_target = lambda x: (1-x[\"decoy\"]).cumsum()))\n",
    "\n",
    "Fig, ax1 = plt.subplots(1,1, figsize=(5,5))\n",
    "ax2 = ax1.twinx()\n",
    "cutoff = after_rescoring_sklearn.where(after_rescoring_sklearn[\"percent_decoy\"] > 1).dropna().head(1)\n",
    "sns.histplot(after_rescoring_sklearn, x=\"logit_p_sklearn\",ax=ax1,hue=\"decoy\")\n",
    "sns.lineplot(data=after_rescoring_sklearn, x=\"logit_p_sklearn\", y=\"percent_decoy\",ax=ax2, color=\"red\")\n",
    "ax1.vlines(cutoff[\"logit_p_sklearn\"], 0, 600, color=\"black\", linestyle=\"--\")\n",
    "ax1.annotate(f\"{cutoff['count_target'].values[0]:.0f} non decoy\", (cutoff[\"logit_p_sklearn\"].values[0]-5, 600), color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(y=X_test.columns, x=clf.coef_.flatten())\n",
    "plt.xlabel(\"Logistic Regression Coefficients\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance (Sklearn)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aki_prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
